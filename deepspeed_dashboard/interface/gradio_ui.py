import time
import requests
import gradio as gr
from app.utils import show_metrics, load_model_config  # åŠ è½½é…ç½®
from run import start_api  # å¯¼å…¥APIå¯åŠ¨å‡½æ•°

# åŠ è½½é…ç½®æ–‡ä»¶é»˜è®¤å€¼
config = load_model_config()
DEFAULT_MODEL = config["model_name"]
DEFAULT_GEN_PARAMS = config["default_generation"]

# å…¨å±€å˜é‡ï¼šè®°å½•APIçº¿ç¨‹
_uvicorn_thread = {"th": None}

def human_time(seconds: float) -> str:
    if seconds < 1:
        return f"{seconds*1000:.1f} æ¯«ç§’"
    return f"{seconds:.2f} ç§’"

def reload_model(model_name: str, device_map: str) -> str:
    try:
        # è°ƒç”¨APIé‡æ–°åŠ è½½æ¨¡å‹
        response = requests.post(
            "http://localhost:8000/reload_model",
            json={"model_name": model_name, "device_map": device_map},
            timeout=120
        )
        response.raise_for_status()
        return f"æ¨¡å‹åŠ è½½æˆåŠŸï¼š{response.json()['model']}ï¼ˆåç«¯ï¼š{response.json()['backend']}ï¼‰"
    except Exception as e:
        return f"åŠ è½½å¤±è´¥ï¼š{str(e)}"

def do_infer(prompt: str, temperature: float, top_k: int, top_p: float, 
             max_new_tokens: int, repetition_penalty: float, do_sample: bool, num_return_sequences: int):
    start = time.time()
    try:
        response = requests.post(
            "http://localhost:8000/generate",
            json={
                "prompt": prompt,
                "temperature": temperature,
                "top_k": top_k,
                "top_p": top_p,
                "max_new_tokens": int(max_new_tokens),
                "repetition_penalty": repetition_penalty,
                "do_sample": do_sample,
                "num_return_sequences": int(num_return_sequences)
            },
            timeout=60
        )
        response.raise_for_status()
        result = response.json()["result"]
        return result, time.time() - start
    except Exception as e:
        return f"æ¨ç†å¤±è´¥ï¼š{str(e)}", time.time() - start

def start_api_service(host: str, port: int) -> str:
    global _uvicorn_thread
    if _uvicorn_thread["th"] is None:
        import threading
        _uvicorn_thread["th"] = threading.Thread(
            target=start_api, args=(host, port), daemon=True
        )
        _uvicorn_thread["th"].start()
        return f"APIæœåŠ¡å·²å¯åŠ¨ï¼šhttp://{host}:{port}"
    return "APIæœåŠ¡å·²åœ¨è¿è¡Œ"

def stop_api_service():
    global _uvicorn_thread
    if _uvicorn_thread["th"]:
        _uvicorn_thread["th"].join(timeout=1.0)  # ä¼˜é›…å…³é—­
        _uvicorn_thread["th"] = None
        return "APIå·²åœæ­¢"
    return "APIæœªè¿è¡Œ"

def run_training(model_t: str, dataset_file, epochs: int, batch_size: int, lr: float, 
                 use_lora: bool, use_zero: bool, max_length: int) -> str:
    from app.training import Trainer
    if not dataset_file:
        return "è¯·ä¸Šä¼ è®­ç»ƒæ•°æ®é›†ï¼ˆJSONæ ¼å¼ï¼‰"
    try:
        trainer = Trainer(
            model_name=model_t,
            dataset_path=dataset_file.name,
            output_dir="checkpoints"
        )
        result = trainer.train(
            epochs=int(epochs),
            batch_size=int(batch_size),
            lr=lr,
            use_lora=use_lora,
            use_zero=use_zero,
            max_length=int(max_length)
        )
        return result
    except Exception as e:
        return f"è®­ç»ƒå¤±è´¥ï¼š{str(e)}"

def run_compress(model_c: str, export_fp16: bool, use_int8: bool, use_int4: bool, 
                 plan_zeroquant: bool, plan_fp6: bool) -> str:
    from app.compression import Compressor
    try:
        compressor = Compressor(model_name=model_c, output_dir="compressed")
        result = compressor.run(
            export_fp16=export_fp16,
            use_int8=use_int8,
            use_int4=use_int4,
            plan_zeroquant=plan_zeroquant,
            plan_fp6=plan_fp6
        )
        return result
    except Exception as e:
        return f"å‹ç¼©å¤±è´¥ï¼š{str(e)}"

def launch_ui():
    with gr.Blocks(title="DeepSpeedä¸­æ–‡æ§åˆ¶å°", theme=gr.themes.Soft()) as demo:
        gr.Markdown("## ğŸš€ DeepSpeedä¸­æ–‡æ§åˆ¶å°ï¼ˆæœ¬åœ°ï¼‰")
        with gr.Row():
            with gr.Column(scale=2):
                gr.Markdown("### æ¨ç†æ§åˆ¶å°")
                model_name_box = gr.Textbox(label="æ¨¡å‹åç§°/è·¯å¾„", value=DEFAULT_MODEL)
                device_map_box = gr.Dropdown(choices=["auto", "cpu"], value=config["device_map"], label="è®¾å¤‡æ˜ å°„")
                reload_btn = gr.Button("åŠ è½½/åˆ‡æ¢æ¨¡å‹")
                reload_log = gr.Textbox(label="åŠ è½½æ—¥å¿—", lines=2)
                reload_btn.click(fn=reload_model, inputs=[model_name_box, device_map_box], outputs=reload_log)

                prompt = gr.Textbox(label="è¾“å…¥æ–‡æœ¬", placeholder="ä¾‹å¦‚ï¼šå†™ä¸€é¦–å…³äºç§‹å¤©çš„è¯—", lines=5)
                with gr.Row():
                    temperature = gr.Slider(0.0, 2.0, value=DEFAULT_GEN_PARAMS["temperature"], step=0.05, label="æ¸©åº¦")
                    top_k = gr.Slider(0, 200, value=DEFAULT_GEN_PARAMS["top_k"], step=1, label="Top-K")
                    top_p = gr.Slider(0.0, 1.0, value=DEFAULT_GEN_PARAMS["top_p"], step=0.01, label="Top-P")
                    repetition_penalty = gr.Slider(0.8, 2.0, value=DEFAULT_GEN_PARAMS["repetition_penalty"], step=0.05, label="é‡å¤æƒ©ç½š")
                with gr.Row():
                    max_new_tokens = gr.Slider(1, 2048, value=DEFAULT_GEN_PARAMS["max_new_tokens"], step=1, label="æœ€å¤§ç”Ÿæˆé•¿åº¦")
                    do_sample = gr.Checkbox(value=True, label="é‡‡æ ·")
                    num_return_sequences = gr.Slider(1, 4, value=1, step=1, label="è¿”å›åºåˆ—æ•°")

                gen_btn = gr.Button("å¼€å§‹æ¨ç†")
                output = gr.Textbox(label="æ¨¡å‹è¾“å‡º", lines=8)
                latency_box = gr.Textbox(label="æ¨ç†è€—æ—¶")
                
                @gr.render(inputs=[prompt, temperature, top_k, top_p, max_new_tokens, repetition_penalty, do_sample, num_return_sequences], triggers=[gen_btn.click])
                def _render_infer(p, t, k, pp, mnt, rp, ds, nrs):
                    text, lat = do_infer(p, t, k, pp, mnt, rp, ds, nrs)
                    return [output.update(value=text), latency_box.update(value=human_time(lat))]

            with gr.Column(scale=1):
                gr.Markdown("### ç³»ç»Ÿç›‘æ§")
                metrics_box = gr.Textbox(label="èµ„æºä¿¡æ¯", lines=20, interactive=False)
                refresh_btn = gr.Button("åˆ·æ–°èµ„æºä¿¡æ¯")
                refresh_btn.click(fn=show_metrics, inputs=None, outputs=metrics_box)

        gr.Markdown("---")
        with gr.Tab("è®­ç»ƒæ¨¡å—"):
            with gr.Row():
                model_t = gr.Textbox(label="æ¨¡å‹åç§°/è·¯å¾„", value=DEFAULT_MODEL)
                dataset_file = gr.File(label="ä¸Šä¼ è®­ç»ƒæ•°æ®ï¼ˆJSONåˆ—è¡¨ï¼Œå«input/outputï¼‰", file_types=[".json", ".jsonl"])  # æ”¯æŒjsonl
            with gr.Row():
                epochs = gr.Slider(1, 3, value=1, step=1, label="è®­ç»ƒè½®æ•°")
                batch_size = gr.Slider(1, 8, value=1, step=1, label="æ‰¹æ¬¡å¤§å°ï¼ˆæ˜¾å­˜ç´§å¼ è¯·ç”¨1ï¼‰")
                max_length = gr.Slider(128, 2048, value=1024, step=64, label="æœ€å¤§åºåˆ—é•¿åº¦")

            with gr.Row():
                lr = gr.Number(label="å­¦ä¹ ç‡", value=5e-5, precision=6)
                use_lora = gr.Checkbox(label="å¯ç”¨LoRA", value=False)
                use_zero = gr.Checkbox(label="å¯ç”¨DeepSpeed ZeROï¼ˆç®€åŒ–ï¼‰", value=False)
            train_btn = gr.Button("å¼€å§‹è®­ç»ƒ")
            train_log = gr.Textbox(label="è®­ç»ƒæ—¥å¿—/ç»“æœ", lines=8)
            train_btn.click(
                fn=run_training,
                inputs=[model_t, dataset_file, epochs, batch_size, lr, use_lora, use_zero, max_length],
                outputs=train_log
            )

        with gr.Tab("å‹ç¼©æ¨¡å—"):
            model_c = gr.Textbox(label="æ¨¡å‹åç§°/è·¯å¾„", value=DEFAULT_MODEL)
            export_fp16 = gr.Checkbox(label="å¯¼å‡ºFP16/åŸç²¾åº¦æƒé‡å‰¯æœ¬", value=True)
            use_int8 = gr.Checkbox(label="ç”Ÿæˆ8bitåŠ è½½æç¤ºï¼ˆbitsandbytesï¼‰", value=False)
            use_int4 = gr.Checkbox(label="ç”Ÿæˆ4bitåŠ è½½æç¤ºï¼ˆbitsandbytesï¼‰", value=False)
            plan_zeroquant = gr.Checkbox(label="è®°å½•ZeroQuantè®¡åˆ’", value=False)
            plan_fp6 = gr.Checkbox(label="è®°å½•FP6è®¡åˆ’", value=False)
            compress_btn = gr.Button("æ‰§è¡Œå‹ç¼©/å¯¼å‡º")
            compress_log = gr.Textbox(label="å‹ç¼©æ—¥å¿—", lines=10)
            compress_btn.click(
                fn=run_compress,
                inputs=[model_c, export_fp16, use_int8, use_int4, plan_zeroquant, plan_fp6],
                outputs=compress_log
            )

        with gr.Tab("APIæœåŠ¡"):
            host = gr.Textbox(label="Host", value="0.0.0.0")
            port = gr.Number(label="Port", value=8000, precision=0)
            api_btn = gr.Button("å¯åŠ¨æœ¬åœ°APIï¼ˆFastAPIï¼‰")
            stop_api_btn = gr.Button("åœæ­¢APIæœåŠ¡")  # æ–°å¢åœæ­¢æŒ‰é’®
            api_log = gr.Textbox(label="çŠ¶æ€", lines=3)
            
            api_btn.click(fn=start_api_service, inputs=[host, port], outputs=api_log)
            stop_api_btn.click(fn=stop_api_service, inputs=None, outputs=api_log)  # ç»‘å®šåœæ­¢å‡½æ•°

        gr.Markdown("æç¤ºï¼šæ¨ç†é»˜è®¤ä¼˜å…ˆä½¿ç”¨DeepSpeed MIIï¼Œä¸å¯ç”¨æ—¶è‡ªåŠ¨å›é€€åˆ°Transformersã€‚è®­ç»ƒä¸ºè½»é‡ç¤ºä¾‹ï¼Œå¤æ‚åœºæ™¯è¯·æŒ‰éœ€æ‰©å±•ã€‚")

    demo.launch()

if __name__ == "__main__":
    launch_ui()
